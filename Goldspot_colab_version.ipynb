{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Goldspot.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "1xAt4CiJfi0q_wfhCi6VFhY9qMzBTnpgh",
      "authorship_tag": "ABX9TyN3iZCcG9cMIOE0hB0Tc7DE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YuannongMao01/CxC_Goldspot/blob/main/Goldspot_colab_version.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CxC Goldspot chellenge\n",
        "### In this project used 3D Convolutional neural networks (3D CNN) based on PyTorch framework"
      ],
      "metadata": {
        "id": "NbOZmKf_oO01"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Util"
      ],
      "metadata": {
        "id": "Esow06mqgKTg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from random import shuffle\n",
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "# import torch.utils.data.Subset \n",
        "from PIL import Image\n",
        "from torchvision.transforms import transforms\n",
        "from torch.utils.data.dataset import Dataset"
      ],
      "metadata": {
        "id": "jklfhExEgOaj"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class IamgesWithLabelsDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Given each of image the corresponding \n",
        "    labels into three class(Fractured, Intact, Shattered)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, root, transform=None) -> None:\n",
        "        super().__init__()\n",
        "        self.label2index = {'Fractured': 0, 'Intact': 1, 'Shattered': 2}  \n",
        "\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.ToTensor(),  \n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225])  # Standardization\n",
        "        ])\n",
        "\n",
        "        root = '/content/drive/MyDrive/GoldSpot_Challenge_Waterloo_Dataset/'\n",
        "\n",
        "        self.root = root\n",
        "\n",
        "        images_path = Path(root + 'images/')\n",
        "\n",
        "        images_list = list(\n",
        "            images_path.glob('*.png'))  # Get the path of each of images\n",
        "        images_list_str = [str(x) for x in images_list]  \n",
        "        images_list_str.sort(\n",
        "            key=lambda item: int(item[len(root + 'images/') - 0:-4]))\n",
        "        self.images = images_list_str\n",
        "\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        \"\"\"\n",
        "        Arg: # of image\n",
        "        Return:\n",
        "            frames:shape->[patches_size,2*step,channel,width,height]\n",
        "            label:shape->[patches_size,....]\n",
        "        \"\"\"\n",
        "        image_path = self.images[item]\n",
        "\n",
        "        labels_path = self.root + 'labels/' + str(item) + '.csv'   \n",
        "        data = pd.read_csv(labels_path, header=0)\n",
        "        id = data.loc[0, 'image_id']  \n",
        "        label = data.loc[:, 'class']  \n",
        "        for i in range(len(label)):\n",
        "            label[i] = self.label2index[label[i]]   #convert strings in class to number\n",
        "        label = torch.tensor(label)\n",
        "\n",
        "        # Segmentation\n",
        "        start_pixel = data.loc[:, 'start_pixel']\n",
        "        end_pixel = data.loc[:, 'end_pixel']\n",
        "\n",
        "        image = Image.open(image_path)  # Read RGB images\n",
        "        width, height = image.size\n",
        "\n",
        "        # Segmentation and resize the images\n",
        "        image_patches = [\n",
        "            image.crop(box=(start, 0, end, height)).resize((128, 171)) \n",
        "            for start, end in zip(start_pixel, end_pixel)   \n",
        "        ]\n",
        "\n",
        "        image_patches = [\n",
        "            self.transform(_image_patch) for _image_patch in image_patches   \n",
        "        ]  \n",
        "\n",
        "        frames = []\n",
        "\n",
        "        step = 1\n",
        "        image_patches = [image_patches[0]] * step + image_patches + [image_patches[-1]] * step \n",
        "\n",
        "        for i in range(step, len(image_patches) - step):\n",
        "            frame = image_patches[i - step:i + step + 1]   #There are 2*step+1 element in list\n",
        "            frame = np.array([item.numpy() for item in frame])\n",
        "            frames.append(frame)  \n",
        "        frames = np.array(frames)\n",
        "\n",
        "        return frames, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n"
      ],
      "metadata": {
        "id": "D-tP17sEgSVF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def onehot2label(onehot):\n",
        "    table = {\n",
        "        0: 'Fractured',\n",
        "        1: 'Intact',\n",
        "        2: 'Shattered',\n",
        "    }\n",
        "\n",
        "    # onehot = onehot[0]\n",
        "    index = onehot.item()\n",
        "    return table[index]"
      ],
      "metadata": {
        "id": "hOJvxfoAgJ9H"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Data(Dataset):\n",
        "    \"\"\"\n",
        "    Converting to Iterative ZIP file\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data) -> None:\n",
        "        super().__init__()\n",
        "        self.data = list(data)\n",
        "        self.data = [list(item) for item in self.data]\n",
        "\n",
        "    def add(self, frame, label):\n",
        "        self.data.append((frame, label))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.data[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n"
      ],
      "metadata": {
        "id": "nJx1jErxhAZ3"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_train_test(data):\n",
        "    dataset_frame = []\n",
        "    dataset_label = []\n",
        "    for frames, labels in data:\n",
        "        for frame, label in zip(frames, labels):\n",
        "            dataset_frame.append(frame)\n",
        "            dataset_label.append(label)\n",
        "\n",
        "    # dataset_frame = np.array(dataset_frame)\n",
        "    # dataset_label = np.array(dataset_label)\n",
        "    # dataset = np.concatenate([dataset_frame, dataset_frame], axis=1)\n",
        "    train_size = int(0.7 * len(dataset_frame))\n",
        "\n",
        "    # Create train/test dataset\n",
        "    train = list(zip(dataset_frame[:train_size], dataset_label[:train_size]))\n",
        "    test = Data(zip(dataset_frame[train_size:], dataset_label[train_size:]))\n",
        "\n",
        "    fractured_num = 0\n",
        "    shattered_num = 0\n",
        "    for i in dataset_label[:train_size]:\n",
        "        if (i == 0): fractured_num += 1\n",
        "        elif (i == 2): shattered_num += 1\n",
        "\n",
        "    fractured_weight = train_size / fractured_num\n",
        "    shattered_weight = train_size / shattered_num\n",
        "    weights = [fractured_weight, shattered_weight]\n",
        "    looptrain = [item for item in train]  \n",
        "    for (frame, label) in looptrain:\n",
        "        for _ in range(\n",
        "                1,\n",
        "                int(weights[0 if 'Fractured' == onehot2label(label) else 1])):\n",
        "            train.append((frame, label))\n",
        "            \n",
        "    # random shuffle\n",
        "    shuffle(train)\n",
        "    train = Data(train)\n",
        "\n",
        "    return train, test"
      ],
      "metadata": {
        "id": "JJH8Z5L8hAk_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3D-CNN Model"
      ],
      "metadata": {
        "id": "9gc9ezOyh00p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class C3D(nn.Module):\n",
        "    \"\"\"\n",
        "    The C3D network.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_classes):\n",
        "        super(C3D, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv3d(3, 64, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
        "        # self.relu = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2))\n",
        "\n",
        "        self.conv2 = nn.Conv3d(64,\n",
        "                               128,\n",
        "                               kernel_size=(3, 3, 3),\n",
        "                               padding=(1, 1, 1))\n",
        "        self.pool2 = nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
        "\n",
        "        self.conv3a = nn.Conv3d(128,\n",
        "                                256,\n",
        "                                kernel_size=(3, 3, 3),\n",
        "                                padding=(1, 1, 1))\n",
        "        self.conv3b = nn.Conv3d(256,\n",
        "                                256,\n",
        "                                kernel_size=(3, 3, 3),\n",
        "                                padding=(1, 1, 1))\n",
        "        self.pool3 = nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(2, 2, 2))\n",
        "\n",
        "        self.conv4a = nn.Conv3d(256,\n",
        "                                512,\n",
        "                                kernel_size=(3, 3, 3),\n",
        "                                padding=(1, 1, 1))\n",
        "        self.conv4b = nn.Conv3d(512,\n",
        "                                512,\n",
        "                                kernel_size=(3, 3, 3),\n",
        "                                padding=(1, 1, 1))\n",
        "        self.pool4 = nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(2, 2, 2))\n",
        "\n",
        "        self.conv5a = nn.Conv3d(512,\n",
        "                                512,\n",
        "                                kernel_size=(2, 3, 3),\n",
        "                                padding=(1, 1, 1))\n",
        "        self.conv5b = nn.Conv3d(512,\n",
        "                                512,\n",
        "                                kernel_size=(3, 3, 3),\n",
        "                                padding=(1, 1, 1))\n",
        "        self.pool5 = nn.MaxPool3d(kernel_size=(2, 2, 2),\n",
        "                                  stride=(2, 2, 2),\n",
        "                                  padding=(0, 1, 1))\n",
        "\n",
        "        self.fc6 = nn.Linear(512 * 6 * 5, 512)\n",
        "        self.fc7 = nn.Linear(512, 100)\n",
        "        self.fc8 = nn.Linear(100, num_classes)\n",
        "\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "\n",
        "        self.relu = nn.ReLU(inplace=True)  \n",
        "\n",
        "        # self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.relu(self.conv1(x))  \n",
        "\n",
        "        x = self.pool1(x)  \n",
        "\n",
        "\n",
        "        x = self.relu(self.conv2(x))  \n",
        "\n",
        "        x = self.pool2(x)  \n",
        "\n",
        "\n",
        "        x = self.relu(self.conv3a(x)) \n",
        "\n",
        "        x = self.relu(self.conv3b(x))  \n",
        "\n",
        "        x = self.pool3(x)  \n",
        "\n",
        "\n",
        "        x = self.relu(self.conv4a(x)) \n",
        "     \n",
        "        x = self.relu(self.conv4b(x))  \n",
        "\n",
        "        x = self.pool4(x)  \n",
        "       \n",
        "\n",
        "        x = self.relu(self.conv5a(x)) \n",
        "  \n",
        "        x = self.relu(self.conv5b(x))  \n",
        "\n",
        "        x = self.pool5(x)  \n",
        "\n",
        "        x = x.view(x.shape[0], 512 * 6 * 5)  \n",
        "\n",
        "        x = self.relu(self.fc6(x))\n",
        "\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(self.fc7(x))\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        logits = self.fc8(x)\n",
        "\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "mV4HRN--hpAk"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the Model"
      ],
      "metadata": {
        "id": "Ej2ozOMhilIa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import torch.optim as optim\n",
        "from torch.utils.data.dataloader import DataLoader"
      ],
      "metadata": {
        "id": "jlq8tfPXh97O"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enables benchmark mode in cudnn\n",
        "# torch.backends.cudnn.benchmark = t = True\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "is_relearn = False\n",
        "VERSION = 2.0   \n",
        "\n",
        "root = '/content/drive/MyDrive/GoldSpot_Challenge_Waterloo_Dataset/'\n",
        "\n",
        "dataset = IamgesWithLabelsDataset(root)   \n",
        "train_dataset, test_dataset = split_train_test(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L20wiiZrjNds",
        "outputId": "6e41697d-8563-424e-b80f-5d1a4c1e8103"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:45: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loader_dataset = DataLoader(train_dataset,\n",
        "                            batch_size=10,   \n",
        "                            shuffle=True,\n",
        "                            num_workers=0)"
      ],
      "metadata": {
        "id": "muGZhqgvmce_"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = C3D(3).to(device)\n",
        "\n",
        "accumulate_steps = 2  \n",
        "lr = 0.003\n",
        "momentum = 0.9\n",
        "EPOCH = 12\n",
        "EPOCH_START = 0\n",
        "average_loss = []\n",
        "if is_relearn:\n",
        "    EPOCH_START = 4\n",
        "    PATH = f'/content/drive/MyDrive/GoldSpot_Challenge_Waterloo_Dataset/{VERSION}-{EPOCH_START}-c3d.pt'\n",
        "    lr = 0.0003\n",
        "    m_state_dict = torch.load(PATH)\n",
        "    model.load_state_dict(m_state_dict)\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(),\n",
        "                      lr=lr,\n",
        "                      momentum=momentum,\n",
        "                      weight_decay=5e-4)  \n",
        "\n",
        "crossEntropyLoss = nn.CrossEntropyLoss()\n",
        "\n",
        "losses = pd.DataFrame([],\n",
        "                      columns=['Train Epoch', 'lr', 'loss', 'average_loss'])\n",
        "lables = []"
      ],
      "metadata": {
        "id": "AHFyhi-ZmhO7"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training the model\n",
        "for epoch in range(EPOCH_START, EPOCH):\n",
        "    if (epoch + 1) % 4 == 0: lr /= 10\n",
        "    pbar = tqdm(loader_dataset, total=len(loader_dataset))\n",
        "    for cnt, batch in enumerate(pbar):\n",
        "        frame, label = batch\n",
        "        frame = frame.permute(0, 2, 1, 3, 4).to(device)\n",
        "        prd = model(frame)\n",
        "        # Expected floating point type for target with class probabilities, got Long\n",
        "        loss = crossEntropyLoss(prd.to('cpu').float(), label)\n",
        "        loss.backward()\n",
        "        average_loss.append(loss.item())\n",
        "        losses = losses.append(\n",
        "            {\n",
        "                'Train Epoch': f'{epoch + 1 } / {EPOCH}',\n",
        "                'lr': lr,\n",
        "                'loss': loss.item(),\n",
        "                'average_loss': round(np.mean(average_loss), 4)\n",
        "            },\n",
        "            ignore_index=True)\n",
        "        pbar.set_description(\n",
        "            f'Train Epoch:{epoch + 1}/{EPOCH} lr:{lr} train_loss:{loss.item()} average_train_loss:{round(np.mean(average_loss), 4)}'\n",
        "        )\n",
        "        \n",
        "        #To solve CUDA error: out of memory\n",
        "        if (cnt + 1) % accumulate_steps == 0:\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "        del loss\n",
        "    if (epoch + 1) % 4 == 0:\n",
        "        # The model are saved every four epochs\n",
        "        PATH = f'/content/drive/MyDrive/GoldSpot_Challenge_Waterloo_Dataset/{VERSION}-{epoch + 1}-c3d.pt'\n",
        "        torch.save(model.state_dict(), PATH)\n",
        "        losses.to_csv(\n",
        "            f'/content/drive/MyDrive/GoldSpot_Challenge_Waterloo_Dataset/{VERSION}-losses-lr-{lr} momentum-{momentum} epoch-{epoch}.csv'\n",
        "        )\n",
        "        del losses\n",
        "        losses = pd.DataFrame(\n",
        "            [], columns=['Train Epoch', 'lr', 'loss', 'average_loss'])\n",
        "        del average_loss\n",
        "        average_loss = []"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sm-FCMPlmuNs",
        "outputId": "69bc21f2-55d9-495f-fb7c-113f4e504e6f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch:1/12 lr:0.003 train_loss:0.747889518737793 average_train_loss:0.8658: 100%|██████████| 255/255 [00:54<00:00,  4.65it/s]\n",
            "Train Epoch:2/12 lr:0.003 train_loss:0.7267696857452393 average_train_loss:0.7972: 100%|██████████| 255/255 [00:57<00:00,  4.46it/s]\n",
            "Train Epoch:3/12 lr:0.003 train_loss:0.7111523151397705 average_train_loss:0.7668: 100%|██████████| 255/255 [00:57<00:00,  4.47it/s]\n",
            "Train Epoch:4/12 lr:0.00030000000000000003 train_loss:0.6944613456726074 average_train_loss:0.7513: 100%|██████████| 255/255 [00:57<00:00,  4.45it/s]\n",
            "Train Epoch:5/12 lr:0.00030000000000000003 train_loss:0.686608076095581 average_train_loss:0.6996: 100%|██████████| 255/255 [00:57<00:00,  4.45it/s]\n",
            "Train Epoch:6/12 lr:0.00030000000000000003 train_loss:0.6827861666679382 average_train_loss:0.701: 100%|██████████| 255/255 [00:57<00:00,  4.47it/s]\n",
            "Train Epoch:7/12 lr:0.00030000000000000003 train_loss:0.6751766800880432 average_train_loss:0.7011: 100%|██████████| 255/255 [00:57<00:00,  4.46it/s]\n",
            "Train Epoch:8/12 lr:3.0000000000000004e-05 train_loss:0.6990344524383545 average_train_loss:0.7005: 100%|██████████| 255/255 [00:57<00:00,  4.46it/s]\n",
            "Train Epoch:9/12 lr:3.0000000000000004e-05 train_loss:0.6985992789268494 average_train_loss:0.6984: 100%|██████████| 255/255 [00:57<00:00,  4.45it/s]\n",
            "Train Epoch:10/12 lr:3.0000000000000004e-05 train_loss:0.668289840221405 average_train_loss:0.6983: 100%|██████████| 255/255 [00:57<00:00,  4.47it/s]\n",
            "Train Epoch:11/12 lr:3.0000000000000004e-05 train_loss:0.6369244456291199 average_train_loss:0.6982: 100%|██████████| 255/255 [00:57<00:00,  4.45it/s]\n",
            "Train Epoch:12/12 lr:3.0000000000000005e-06 train_loss:0.7107447981834412 average_train_loss:0.6986: 100%|██████████| 255/255 [00:57<00:00,  4.45it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing the model"
      ],
      "metadata": {
        "id": "AVK5gu9bmz50"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# EPOCH:12 VERSION:2.0——Accuracy is 36.8%\n",
        "# EPOCH:16 VERSION:2.0—— Accuracy is 18%\n",
        "# EPOCH:8 VERSION:3.0——Accuracy is 19%    \n",
        "EPOCH = 12\n",
        "VERSION = 2.0\n",
        "\n",
        "dataset = IamgesWithLabelsDataset('/content/drive/MyDrive/GoldSpot_Challenge_Waterloo_Dataset/')\n",
        "train_dataset, test_dataset = split_train_test(dataset)\n",
        "loader_dataset = DataLoader(test_dataset,\n",
        "                            batch_size=1,\n",
        "                            shuffle=True,\n",
        "                            num_workers=0)\n",
        "\n",
        "model = C3D(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8ClNLqLm-Ag",
        "outputId": "29ace77a-98b4-432a-d22f-b974274c1587"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:45: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the model\n",
        "PATH = f'/content/drive/MyDrive/GoldSpot_Challenge_Waterloo_Dataset/{VERSION}-{EPOCH}-c3d.pt'\n",
        "m_state_dict = torch.load(PATH)\n",
        "model.load_state_dict(m_state_dict)\n",
        "\n",
        "pbar = tqdm(loader_dataset, total=len(loader_dataset))\n",
        "\n",
        "T = 0\n",
        "A = 0\n",
        "for cnt, batch in enumerate(pbar):\n",
        "    frame, label = batch\n",
        "    frame = frame.permute(0, 2, 1, 3, 4)\n",
        "    prd = F.softmax(model(frame))\n",
        "    target = onehot2label(label[0])\n",
        "    source = onehot2label(torch.argmax(prd[0]))\n",
        "    if (target == source): T += 1\n",
        "    A += 1\n",
        "    pbar.set_description(f'True:{target},Predict:{source},Accuracy:{T*100/A}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUg4-ApPm-lb",
        "outputId": "a2e12eab-56ea-4705-af4d-fbf306f4aff8"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/577 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  del sys.path[0]\n",
            "True:Fractured,Predict:Fractured,Accuracy:67.24436741767764%: 100%|██████████| 577/577 [02:20<00:00,  4.11it/s]\n"
          ]
        }
      ]
    }
  ]
}